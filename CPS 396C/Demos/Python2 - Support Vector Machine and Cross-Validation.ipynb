{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines and Cross-Validation\n",
    "=====\n",
    "\n",
    "Some of the code contained within this notebook is from Ch. 16 of *Data Science from Scratch* by J. Grus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of feature wise normalization.  Notice that the normalization terms are determined only using the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.         0.18       0.07344184 0.         0.31481481 0.57750527\n",
      " 0.64160659 0.26920314 0.         0.22755741 0.28723404 1.\n",
      " 0.08967991]\n",
      "0.0 1.0 -0.21613002146580806 1.0939457202505218\n",
      "(404, 13) (102, 13)\n",
      "3.310518415536489\n",
      "4.730017250961031\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "#print type(X), X.shape, type(y), y.shape\n",
    "N = int(0.8 * X.shape[0])\n",
    "train_X = X[:N,:]\n",
    "train_y = y[:N]\n",
    "val_X = X[N:,:]\n",
    "val_y = y[N:]\n",
    "\n",
    "\n",
    "# Normalize each feature\n",
    "mins_X = np.min(train_X, axis = 0)\n",
    "train_X = train_X - mins_X\n",
    "val_X = val_X - mins_X\n",
    "\n",
    "maxs_X = np.max(train_X, axis = 0)\n",
    "train_X = train_X / maxs_X\n",
    "val_X = val_X / maxs_X\n",
    "\n",
    "print train_X[0,:]\n",
    "\n",
    "print np.min(train_X), np.max(train_X), np.min(val_X), np.max(val_X)\n",
    "\n",
    "print train_X.shape, val_X.shape\n",
    "\n",
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "\n",
    "print mean_absolute_error(reg.predict(train_X), train_y)\n",
    "print mean_absolute_error(reg.predict(val_X), val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a SVM on two classes from the iris dataset.  Try to split off 20% as a validiation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data 1.0\n",
      "Accuracy on training data 1.0\n",
      "Accuracy on validation data 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:100,:]  # we only take the first two classes.\n",
    "y = iris.target[:100]\n",
    "\n",
    "# randomize the data\n",
    "mapping = range(X.shape[0])\n",
    "random.shuffle(mapping)\n",
    "\n",
    "# Remap data\n",
    "X = X[mapping,:]\n",
    "y = y[mapping]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print \"Accuracy on training data\", accuracy_score(clf.predict(X), y)\n",
    "\n",
    "# Create a 80-20 split and evaluate on the validation data\n",
    "N = int(0.8 * X.shape[0])\n",
    "train_X = X[:N,:]\n",
    "train_y = y[:N]\n",
    "val_X = X[N:,:]\n",
    "val_y = y[N:]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print \"Accuracy on training data\", accuracy_score(clf.predict(train_X), train_y)\n",
    "print \"Accuracy on validation data\", accuracy_score(clf.predict(val_X), val_y)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a SVM on two digits form the digit class.  Evaluate using 10 fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "182\n",
      "Accuracy on training data [full data]  1.0\n",
      "Accuracy on training data [80 split] 1.0\n",
      "Accuracy on validation data [20 split] 0.5\n",
      "Accuracy on validation folds\n",
      "[0.0, 0.5555555555555556, 0.6666666666666666, 0.5555555555555556, 0.5555555555555556, 0.4444444444444444, 0.4444444444444444, 0.3333333333333333, 0.6666666666666666, 0.7777777777777778]\n",
      "Avg. over all validation folds 0.5\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn import svm\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# import some data to play with\n",
    "X, y = datasets.load_digits(n_class = 2, return_X_y=True)\n",
    "\n",
    "X = X.reshape((-1, 16*16))\n",
    "\n",
    "print np.sum(y)\n",
    "\n",
    "# randomize the data\n",
    "mapping = range(X.shape[0])\n",
    "random.shuffle(mapping)\n",
    "\n",
    "# Remap data\n",
    "X = X[mapping,:]\n",
    "y = y[mapping]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(X, y)\n",
    "\n",
    "print \"Accuracy on training data [full data] \", accuracy_score(clf.predict(X), y)\n",
    "\n",
    "# Create a 80-20 split and evaluate on the validation data\n",
    "N = int(0.8 * X.shape[0])\n",
    "train_X = X[:N,:]\n",
    "train_y = y[:N]\n",
    "val_X = X[N:,:]\n",
    "val_y = y[N:]\n",
    "\n",
    "clf = svm.SVC()\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print \"Accuracy on training data [80 split]\", accuracy_score(clf.predict(train_X), train_y)\n",
    "print \"Accuracy on validation data [20 split]\", accuracy_score(clf.predict(val_X), val_y)\n",
    "\n",
    "# Do 10-fold cross validation\n",
    "k = 10\n",
    "\n",
    "from sklearn.model_selection import KFold \n",
    "# https://towardsdatascience.com/train-test-split-and-cross-validation-in-python-80b61beca4b6\n",
    "kf = KFold(n_splits=k) \n",
    "kf.get_n_splits(X)\n",
    "\n",
    "fold_accuracies = []\n",
    "\n",
    "for train_index, val_index in kf.split(X):\n",
    "\n",
    "    train_X = X[train_index,:]\n",
    "    train_y = y[train_index]\n",
    "    val_X = X[val_index,:]\n",
    "    val_y = y[val_index]\n",
    "    \n",
    "    clf = svm.SVC(C=1, gamma=0.0001)\n",
    "    #clf = svm.SVC()\n",
    "    clf.fit(train_X, train_y)\n",
    "    \n",
    "    fold_accuracies.append(accuracy_score(clf.predict(val_X), val_y))\n",
    "\n",
    "print \"Accuracy on validation folds\"\n",
    "print fold_accuracies\n",
    "\n",
    "print \"Avg. over all validation folds\", sum(fold_accuracies) / k\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a grid search over gamma and C for the digit classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters set found on development set: {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.528 (+/-0.115) for {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.01}\n",
      "0.569 (+/-0.219) for {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.001}\n",
      "0.583 (+/-0.411) for {'kernel': 'rbf', 'C': 0.1, 'gamma': 0.0001}\n",
      "0.528 (+/-0.115) for {'kernel': 'rbf', 'C': 0.5, 'gamma': 0.01}\n",
      "0.569 (+/-0.219) for {'kernel': 'rbf', 'C': 0.5, 'gamma': 0.001}\n",
      "0.583 (+/-0.388) for {'kernel': 'rbf', 'C': 0.5, 'gamma': 0.0001}\n",
      "0.556 (+/-0.142) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.01}\n",
      "0.569 (+/-0.275) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.001}\n",
      "0.583 (+/-0.484) for {'kernel': 'rbf', 'C': 1, 'gamma': 0.0001}\n",
      "0.556 (+/-0.142) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.01}\n",
      "0.583 (+/-0.276) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.001}\n",
      "0.653 (+/-0.473) for {'kernel': 'rbf', 'C': 10, 'gamma': 0.0001}\n",
      "0.556 (+/-0.142) for {'kernel': 'rbf', 'C': 50, 'gamma': 0.01}\n",
      "0.583 (+/-0.276) for {'kernel': 'rbf', 'C': 50, 'gamma': 0.001}\n",
      "0.653 (+/-0.473) for {'kernel': 'rbf', 'C': 50, 'gamma': 0.0001}\n",
      "0.556 (+/-0.142) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.01}\n",
      "0.583 (+/-0.276) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.001}\n",
      "0.653 (+/-0.473) for {'kernel': 'rbf', 'C': 100, 'gamma': 0.0001}\n",
      "0.556 (+/-0.142) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.01}\n",
      "0.583 (+/-0.276) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.001}\n",
      "0.653 (+/-0.473) for {'kernel': 'rbf', 'C': 1000, 'gamma': 0.0001}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/sklearn/model_selection/_search.py:841: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(0, 1),\n",
       " (0, 1),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (1, 0),\n",
       " (1, 0),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 1),\n",
       " (0, 1),\n",
       " (0, 0),\n",
       " (0, 0),\n",
       " (0, 0)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "tuned_parameters = [{'kernel': ['rbf'], 'gamma': [1e-2, 1e-3, 1e-4],\n",
    "                     'C': [0.1, 0.5, 1, 10, 50, 100, 1000]}]\n",
    "\n",
    "# import some data to play with\n",
    "X, y = datasets.load_digits(n_class = 2, return_X_y=True)\n",
    "\n",
    "X = X.reshape((-1, 16*16))\n",
    "\n",
    "# randomize the data\n",
    "mapping = range(X.shape[0])\n",
    "random.shuffle(mapping)\n",
    "\n",
    "# Remap data\n",
    "X = X[mapping,:]\n",
    "y = y[mapping]\n",
    "\n",
    "# Create a 80-20 split, then do 10-fold cv with grid search and evaluate on the validation data\n",
    "N = int(0.8 * X.shape[0])\n",
    "train_X = X[:N,:]\n",
    "train_y = y[:N]\n",
    "val_X = X[N:,:]\n",
    "val_y = y[N:]\n",
    "\n",
    "clf = GridSearchCV(SVC(), tuned_parameters, cv=10)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print \"Best parameters set found on development set:\", clf.best_params_\n",
    "\n",
    "means = clf.cv_results_['mean_test_score']\n",
    "stds = clf.cv_results_['std_test_score']\n",
    "for mean, std, params in zip(means, stds, clf.cv_results_['params']):\n",
    "    print(\"%0.3f (+/-%0.03f) for %r\" % (mean, std * 2, params))\n",
    "\n",
    "preds = clf.predict(val_X)\n",
    "zip(preds, val_y)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
