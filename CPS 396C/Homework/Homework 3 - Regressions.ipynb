{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Homework 3 - Regresssions\n",
    "=====\n",
    "\n",
    "**Due:** March 22, 2019 by 11:59PM via Blackboard\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_NAME = \"Chris Phillips\" # <-- Your name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Continue the feature selection process for the Boston Housing dataset.  Greedily select the best 3 features and evaluate the performance.  Is the performance better or worse then if all of the features are used?  Split the data into a training and validation dataset and evaluate both [Hint: Copy over and continue code from the lab]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (6.54450145202319, 6.080929821042619)\n",
      "1 (6.254188259087119, 7.0316161242486315)\n",
      "2 (6.175264325332599, 5.011468954357161)\n",
      "3 (6.77241514395342, 8.05896700143472)\n",
      "4 (6.4500896291443395, 5.699073737091736)\n",
      "5 (3.904394479751444, 7.06094634322534)\n",
      "6 (6.4327333258954775, 6.711889732232196)\n",
      "7 (6.7231709925124346, 7.5202012562811635)\n",
      "8 (6.702527550682737, 5.2393799182295036)\n",
      "9 (6.385343483090547, 4.306280609762139)\n",
      "10 (5.991213011661856, 5.45663431489543)\n",
      "11 (6.744151610668998, 5.401079183473508)\n",
      "12 (4.855105474674045, 3.7153181075195643)\n",
      "Best is 12\n",
      "-------\n",
      "0 12 (4.852984352935281, 3.656502213506232)\n",
      "1 12 (4.821917890519309, 3.6305325779158406)\n",
      "2 12 (4.86247179611612, 3.7815766252916707)\n",
      "3 12 (4.739192407784038, 3.5223765950957744)\n",
      "4 12 (4.864154127145216, 4.155276161233407)\n",
      "5 12 (3.801964159106713, 4.987002723682361)\n",
      "6 12 (4.808550904263029, 4.122281739573801)\n",
      "7 12 (4.766075205031166, 4.407518032588313)\n",
      "8 12 (4.869397194850256, 4.248970413678763)\n",
      "9 12 (4.83087570320003, 3.456127334865605)\n",
      "10 12 (4.44770797558722, 2.8770627416127232)\n",
      "11 12 (4.85015801277672, 3.502846738901483)\n",
      "Best is 10 12\n",
      "-------\n",
      "0 10 12 (4.451196710420592, 2.9139353680481928)\n",
      "1 10 12 (4.454011030502268, 2.898343793267648)\n",
      "2 10 12 (4.45357081035381, 2.981936169838191)\n",
      "3 10 12 (4.384234825347898, 2.8446249745236227)\n",
      "4 10 12 (4.43756001069733, 3.036452214494819)\n",
      "5 10 12 (3.547825564538207, 4.237794645617616)\n",
      "6 10 12 (4.315628993020991, 3.252995224881025)\n",
      "7 10 12 (4.235356928949947, 3.3441821256216895)\n",
      "8 10 12 (4.4801684019243, 3.784455785131465)\n",
      "9 10 12 (4.456284760688648, 2.964863831742816)\n",
      "11 10 12 (4.433326748535195, 3.046438383668195)\n",
      "Best is 3 10 12\n",
      "-------\n",
      "Best of all is 3 10 12 with performances of (4.384234825347898, 2.8446249745236227)\n",
      "All features: (3.3105184155364853, 4.730017250960962)\n",
      "-------\n",
      "Using all features, there are better results on the training data but the best validation results are using the three selected features\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def mean_difference(a, b):\n",
    "    return np.sqrt(np.square((a - b))).sum() / a.shape[0]\n",
    "\n",
    "### Bring in the Boston Housing Dataset\n",
    "from sklearn.datasets import load_boston\n",
    "X, y = load_boston(return_X_y=True)\n",
    "\n",
    "N = int(0.8 * X.shape[0])\n",
    "\n",
    "train_X = X[:N,:]\n",
    "train_y = y[:N]\n",
    "val_X = X[N:,:]\n",
    "val_y = y[N:]\n",
    "\n",
    "### Use sklearn's LinearRegression module\n",
    "from sklearn.linear_model import LinearRegression\n",
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate_model(train_X, train_y, val_X, val_y):\n",
    "    reg = LinearRegression().fit(train_X, train_y)\n",
    "    train_y_hat = reg.predict(train_X)\n",
    "    val_y_hat = reg.predict(val_X)\n",
    "    \n",
    "    return (mean_difference(train_y, train_y_hat), mean_difference(val_y, val_y_hat))\n",
    "\n",
    "for i in range(0, X.shape[1]):\n",
    "    sub_train_X = train_X[:,i].reshape((-1,1))\n",
    "    sub_val_X = val_X[:,i].reshape((-1,1))\n",
    "    \n",
    "    print i, train_and_evaluate_model(sub_train_X, train_y, sub_val_X, val_y)\n",
    "\n",
    "first_feat_selected_idx = 12\n",
    "print \"Best is {}\".format(first_feat_selected_idx)\n",
    "\n",
    "print \"-------\"\n",
    "for i in [i for i in range(0, X.shape[1]) if i != first_feat_selected_idx]:\n",
    "    sub_train_X = train_X[:, [i,first_feat_selected_idx]]\n",
    "    sub_val_X = val_X[:, [i, first_feat_selected_idx]]\n",
    "    \n",
    "    print i, first_feat_selected_idx, train_and_evaluate_model(sub_train_X, train_y, sub_val_X, val_y)\n",
    "\n",
    "second_feat_selected_idx = 10\n",
    "print \"Best is {} {}\".format(second_feat_selected_idx, first_feat_selected_idx)\n",
    "\n",
    "print \"-------\"\n",
    "for i in [i for i in range(0, X.shape[1]) if i != first_feat_selected_idx and i != second_feat_selected_idx]:\n",
    "    sub_train_X = train_X[:, [i,first_feat_selected_idx,second_feat_selected_idx]]\n",
    "    sub_val_X = val_X[:, [i, first_feat_selected_idx,second_feat_selected_idx]]\n",
    "    \n",
    "    print i, second_feat_selected_idx, first_feat_selected_idx, train_and_evaluate_model(sub_train_X, train_y, sub_val_X, val_y)\n",
    "\n",
    "third_feat_selected_idx = 3\n",
    "print \"Best is {} {} {}\".format(third_feat_selected_idx, second_feat_selected_idx, first_feat_selected_idx)\n",
    "print \"-------\"\n",
    "\n",
    "best_train_X = train_X[:, [third_feat_selected_idx, first_feat_selected_idx, second_feat_selected_idx]]\n",
    "best_val_X = val_X[:, [third_feat_selected_idx, first_feat_selected_idx, second_feat_selected_idx]]\n",
    "\n",
    "print \"Best of all is {} {} {} with performances of {}\".format(third_feat_selected_idx, second_feat_selected_idx, first_feat_selected_idx, train_and_evaluate_model(best_train_X, train_y, best_val_X, val_y))\n",
    "print \"All features: {}\".format(train_and_evaluate_model(train_X, train_y, val_X, val_y))\n",
    "print \"-------\"\n",
    "print \"Using all features, there are better results on the training data but the best validation results are using the three selected features\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Instead of doing a simple linear regression, do a [multi-layer perceptron regressor](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPRegressor.html#sklearn.neural_network.MLPRegressor).\n",
    "\n",
    "Try a few combinations for hyperparameters.  \n",
    "\n",
    "Compare the performance with the simple linear models.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20.425220755640563, 12.274968377461565)\n",
      "(6.616374314114865, 7.648323461175828)\n",
      "(6.9174964961717835, 12.543814119733643)\n",
      "(6.683721248554281, 5.819434380286091)\n",
      "-------\n",
      "These results are not great and cannot compete with the results of the earlier linear models\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "def train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, activation, learning_rate, max_iter, alpha):\n",
    "    reg = MLPRegressor( hidden_layer_sizes=(5,),\n",
    "                        activation=activation,\n",
    "                        learning_rate=learning_rate,\n",
    "                        max_iter=max_iter,\n",
    "                        learning_rate_init=0.01,\n",
    "                        alpha=alpha).fit(train_X, train_y)\n",
    "\n",
    "    train_y_hat = reg.predict(train_X)\n",
    "    val_y_hat = reg.predict(val_X)\n",
    "    \n",
    "    return (mean_difference(train_y, train_y_hat), mean_difference(val_y, val_y_hat))\n",
    "\n",
    "    \n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'relu', 'constant', 200, 0)\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'logistic', 'adaptive', 500, 0)\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'identity', 'invscaling', 700, 0)\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'logistic', 'constant', 1000, 0)\n",
    "print \"-------\"\n",
    "print \"These results are not great and cannot compete with the results of the earlier linear models\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Develop additional models based on a multi-layer perceptron regressor and a simple linear regressor that use regularization.  How do these compare with the unregularized versions?  Evaluate on both the training and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Un-Regularized\n",
      "\n",
      "All:  (3.3105184155364853, 4.730017250960962)\n",
      "Best:  (4.384234825347898, 2.8446249745236227)\n",
      "\n",
      "Linear Regularized\n",
      "\n",
      "All:  (3.288980113806701, 4.59466355384558)\n",
      "Best:  (4.3832030494943925, 2.8453914699339897)\n",
      "\n",
      "MLP Un-Regularized\n",
      "\n",
      "(4.544949572462268, 18.88455100211753)\n",
      "(6.58673564425691, 7.465512011619283)\n",
      "\n",
      "MLP Regularized\n",
      "\n",
      "(11.610195646870842, 7.140673715381612)\n",
      "(6.734046179870301, 8.17755180523719)\n",
      "\n",
      "Generally, it appears that regularizing the model does not impact the model in a positive way. With the MLP, regularization is drastically inconsistent. With the linear model, there were only minor improvements.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "def train_and_evaluate_regularized_model(train_X, train_y, val_X, val_y):\n",
    "    reg = Ridge().fit(train_X, train_y)\n",
    "\n",
    "    train_y_hat = reg.predict(train_X)\n",
    "    val_y_hat = reg.predict(val_X)\n",
    "    \n",
    "    return (mean_difference(train_y, train_y_hat), mean_difference(val_y, val_y_hat))\n",
    "\n",
    "print \"Linear Un-Regularized\\n\"\n",
    "print \"All: \", train_and_evaluate_model(train_X, train_y, val_X, val_y)\n",
    "print \"Best: \", train_and_evaluate_model(best_train_X, train_y, best_val_X, val_y)\n",
    "\n",
    "print \"\\nLinear Regularized\\n\"\n",
    "print \"All: \", train_and_evaluate_regularized_model(train_X, train_y, val_X, val_y)\n",
    "print \"Best: \", train_and_evaluate_regularized_model(best_train_X, train_y, best_val_X, val_y)\n",
    "\n",
    "print \"\\nMLP Un-Regularized\\n\"\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'relu', 'constant', 200, 0)\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'logistic', 'adaptive', 500, 0)\n",
    "\n",
    "print \"\\nMLP Regularized\\n\"\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'relu', 'constant', 200, 0.005)\n",
    "print train_and_evaluate_MLP_model(train_X, train_y, val_X, val_y, 'logistic', 'adaptive', 500, 0.005)\n",
    "\n",
    "print \"\\nGenerally, it appears that regularizing the model does not impact the model in a positive way. \" \\\n",
    "        \"With the MLP, regularization is drastically inconsistent. With the linear model, there were only minor improvements.\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
