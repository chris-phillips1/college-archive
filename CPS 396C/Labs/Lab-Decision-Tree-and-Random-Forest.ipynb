{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree and Random Forest\n",
    "=====\n",
    "\n",
    "**Objective**: Build and tune a decision tree for a classification problem.  Build and evaluate a random forest for a classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "MY_NAME = \"Chris Phillips\" # <-- Your name here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) Bring in some data over ground cover types.  A full description of the dataset can be found at [UCI Machine Learning Dataset](https://archive.ics.uci.edu/ml/datasets/covertype).  It contains 54 attributes that characterize a location in a forest and the target is the type of tree that is found at that location.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _covtype_dataset:\n",
      "\n",
      "Forest covertypes\n",
      "-----------------\n",
      "\n",
      "The samples in this dataset correspond to 30Ã—30m patches of forest in the US,\n",
      "collected for the task of predicting each patch's cover type,\n",
      "i.e. the dominant species of tree.\n",
      "There are seven covertypes, making this a multiclass classification problem.\n",
      "Each sample has 54 features, described on the\n",
      "`dataset's homepage <http://archive.ics.uci.edu/ml/datasets/Covertype>`__.\n",
      "Some of the features are boolean indicators,\n",
      "while others are discrete or continuous measurements.\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      "    =================   ============\n",
      "    Classes                        7\n",
      "    Samples total             581012\n",
      "    Dimensionality                54\n",
      "    Features                     int\n",
      "    =================   ============\n",
      "\n",
      ":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;\n",
      "it returns a dictionary-like object\n",
      "with the feature matrix in the ``data`` member\n",
      "and the target values in ``target``.\n",
      "The dataset will be downloaded from the web if necessary.\n",
      "\n",
      "[[2.596e+03 5.100e+01 3.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.590e+03 5.600e+01 2.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.804e+03 1.390e+02 9.000e+00 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " ...\n",
      " [2.386e+03 1.590e+02 1.700e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.384e+03 1.700e+02 1.500e+01 ... 0.000e+00 0.000e+00 0.000e+00]\n",
      " [2.383e+03 1.650e+02 1.300e+01 ... 0.000e+00 0.000e+00 0.000e+00]]\n",
      "[5 5 2 ... 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "import sklearn.datasets\n",
    "\n",
    "covetype = sklearn.datasets.fetch_covtype()\n",
    "\n",
    "print covetype.DESCR\n",
    "print covetype.data\n",
    "print covetype.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Shuffle the data and split into three seperate datasets, using a 60-20-20 split.  Check the distribution of the classes across the three datasets to see if they are balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(348606, 54) (348606,) (116203, 54) (116203,) (116203, 54) (116203,)\n",
      "Counter({2: 193927, 1: 103044, 3: 23260, 6: 12403, 5: 7497, 7: 5728, 4: 2747})\n",
      "Counter({1: 56740, 2: 42480, 7: 7978, 3: 4798, 6: 2448, 5: 1759})\n",
      "Counter({1: 52056, 2: 46894, 3: 7696, 7: 6804, 6: 2516, 5: 237})\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "X = covetype.data\n",
    "y = covetype.target\n",
    "\n",
    "idx = random.shuffle(range(X.shape[0]))\n",
    "X[idx,:] = X\n",
    "y[idx] = y\n",
    "\n",
    "N = int(0.8 * X.shape[0])\n",
    "\n",
    "tv_X = X[:N,:]\n",
    "tv_y = y[:N]\n",
    "test_X = X[N:,:]\n",
    "test_y = y[N:]\n",
    "\n",
    "N = int(0.75 * tv_X.shape[0])\n",
    "train_X = tv_X[:N,:]\n",
    "train_y = tv_y[:N]\n",
    "val_X = tv_X[N:,:]\n",
    "val_y = tv_y[N:]\n",
    "\n",
    "print train_X.shape, train_y.shape, val_X.shape, val_y.shape, test_X.shape, test_y.shape\n",
    "\n",
    "# Check if classes are balanced across the datasets\n",
    "train_class_counts = collections.Counter(train_y)\n",
    "val_class_counts = collections.Counter(val_y)\n",
    "test_class_counts = collections.Counter(test_y)\n",
    "\n",
    "print train_class_counts\n",
    "print val_class_counts\n",
    "print test_class_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) Now we will build a decision tree for this problem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data ... 1.0\n",
      "Accuracy on validation data ... 0.687615638150478\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# Build and fit tree\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print \"Accuracy on training data ...\", clf.score(train_X, train_y)\n",
    "print \"Accuracy on validation data ...\", clf.score(val_X, val_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4) There is clearly some overfitting taking place.  Let's try to minimize that and also tune the tree at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gini 5 best\n",
      "Accuracy on training data ... 0.7506353878017016\n",
      "Accuracy on validation data ... 0.6160770375979966\n",
      "gini 5 random\n",
      "Accuracy on training data ... 0.6507260345490324\n",
      "Accuracy on validation data ... 0.4049809385299863\n",
      "gini 7 best\n",
      "Accuracy on training data ... 0.7719115563128575\n",
      "Accuracy on validation data ... 0.6584167362288409\n",
      "gini 7 random\n",
      "Accuracy on training data ... 0.7404318915910799\n",
      "Accuracy on validation data ... 0.6166105866457837\n",
      "gini 9 best\n",
      "Accuracy on training data ... 0.8005398644888498\n",
      "Accuracy on validation data ... 0.6680464359784171\n",
      "gini 9 random\n",
      "Accuracy on training data ... 0.7561200897288056\n",
      "Accuracy on validation data ... 0.5939691746340456\n",
      "gini 11 best\n",
      "Accuracy on training data ... 0.8339902353946863\n",
      "Accuracy on validation data ... 0.6472896568935398\n",
      "gini 11 random\n",
      "Accuracy on training data ... 0.7516422551533823\n",
      "Accuracy on validation data ... 0.6756796296136933\n",
      "entropy 5 best\n",
      "Accuracy on training data ... 0.7514357182607299\n",
      "Accuracy on validation data ... 0.5553901362271197\n",
      "entropy 5 random\n",
      "Accuracy on training data ... 0.6923518241223616\n",
      "Accuracy on validation data ... 0.4489126786743888\n",
      "entropy 7 best\n",
      "Accuracy on training data ... 0.7680074353281355\n",
      "Accuracy on validation data ... 0.6424532929442441\n",
      "entropy 7 random\n",
      "Accuracy on training data ... 0.7372879411140371\n",
      "Accuracy on validation data ... 0.6231938934450918\n",
      "entropy 9 best\n",
      "Accuracy on training data ... 0.7945072660826262\n",
      "Accuracy on validation data ... 0.6635198747020301\n",
      "entropy 9 random\n",
      "Accuracy on training data ... 0.7691835481890731\n",
      "Accuracy on validation data ... 0.6790014027176579\n",
      "entropy 11 best\n",
      "Accuracy on training data ... 0.8331210593047739\n",
      "Accuracy on validation data ... 0.6436494754868635\n",
      "entropy 11 random\n",
      "Accuracy on training data ... 0.7914751897557701\n",
      "Accuracy on validation data ... 0.6588125951997797\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "for (criterion, max_depth, splitter) in itertools.product([\"gini\", \"entropy\"], [5, 7, 9, 11], [\"best\", \"random\"]): \n",
    "    \n",
    "    clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, splitter=splitter, random_state=0)\n",
    "    clf.fit(train_X, train_y)\n",
    "\n",
    "    print criterion, max_depth, splitter\n",
    "    print \"Accuracy on training data ...\", clf.score(train_X, train_y)\n",
    "    print \"Accuracy on validation data ...\", clf.score(val_X, val_y)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5) Select the best set of hyperparameters, recreate the tree and evaluate the performance by class.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data ... 0.7691835481890731\n",
      "Accuracy on validation data ... 0.6790014027176579\n",
      "[[35521 16893    28    15     0  4283]\n",
      " [ 8593 32996   346     2   269   274]\n",
      " [    0  1391  2990    41   376     0]\n",
      " [    0  1566   128    38    27     0]\n",
      " [    0  1251   291     7   899     0]\n",
      " [ 1443    77     0     0     0  6458]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.78      0.63      0.69     56740\n",
      "           2       0.61      0.78      0.68     42480\n",
      "           3       0.79      0.62      0.70      4798\n",
      "           5       0.37      0.02      0.04      1759\n",
      "           6       0.57      0.37      0.45      2448\n",
      "           7       0.59      0.81      0.68      7978\n",
      "\n",
      "   micro avg       0.68      0.68      0.68    116203\n",
      "   macro avg       0.62      0.54      0.54    116203\n",
      "weighted avg       0.69      0.68      0.67    116203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "criterion = 'entropy'\n",
    "max_depth = 9\n",
    "splitter = 'random'\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=criterion, max_depth=max_depth, splitter=splitter, random_state=0)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print \"Accuracy on training data ...\", clf.score(train_X, train_y)\n",
    "print \"Accuracy on validation data ...\", clf.score(val_X, val_y)\n",
    "\n",
    "pred_y = clf.predict(val_X)\n",
    "\n",
    "print confusion_matrix(val_y, pred_y)\n",
    "print classification_report(val_y, pred_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6) Finally, let's build a random forest built from a number of decision trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on training data ... 0.9977883341078467\n",
      "Accuracy on validation data ... 0.7621059697253943\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Build and fit forest here\n",
    "clf = RandomForestClassifier(n_estimators=30, random_state=0, max_depth=30)\n",
    "clf.fit(train_X, train_y)\n",
    "\n",
    "print \"Accuracy on training data ...\", clf.score(train_X, train_y)\n",
    "print \"Accuracy on validation data ...\", clf.score(val_X, val_y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test data ... 0.7046289682710429\n",
      "[[37926 10717    73   111   169  3060]\n",
      " [13482 28247  1707   823  1983   652]\n",
      " [    5   442  6664     5   580     0]\n",
      " [   17    88     4   128     0     0]\n",
      " [    1    99   178     2  2236     0]\n",
      " [  120     5     0     0     0  6679]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.73      0.73     52056\n",
      "           2       0.71      0.60      0.65     46894\n",
      "           3       0.77      0.87      0.82      7696\n",
      "           5       0.12      0.54      0.20       237\n",
      "           6       0.45      0.89      0.60      2516\n",
      "           7       0.64      0.98      0.78      6804\n",
      "\n",
      "   micro avg       0.70      0.70      0.70    116203\n",
      "   macro avg       0.57      0.77      0.63    116203\n",
      "weighted avg       0.72      0.70      0.70    116203\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy on test data ...\", clf.score(test_X, test_y)\n",
    "\n",
    "pred_y = clf.predict(test_X)\n",
    "\n",
    "print confusion_matrix(test_y, pred_y)\n",
    "print classification_report(test_y, pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
